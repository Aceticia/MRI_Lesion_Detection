
@online{brownLanguageModelsAre2020,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  date = {2020-07-22},
  eprint = {2005.14165},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2005.14165},
  urldate = {2022-02-20},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/xujinliu/Zotero/storage/3XLJPF7W/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf;/Users/xujinliu/Zotero/storage/EBS3Q6N2/2005.html}
}

@article{commowickObjectiveEvaluationMultiple2018,
  title = {Objective {{Evaluation}} of {{Multiple Sclerosis Lesion Segmentation}} Using a {{Data Management}} and {{Processing Infrastructure}}},
  author = {Commowick, Olivier and Istace, Audrey and Kain, Michaël and Laurent, Baptiste and Leray, Florent and Simon, Mathieu and Pop, Sorina Camarasu and Girard, Pascal and Améli, Roxana and Ferré, Jean-Christophe and Kerbrat, Anne and Tourdias, Thomas and Cervenansky, Frédéric and Glatard, Tristan and Beaumont, Jérémy and Doyle, Senan and Forbes, Florence and Knight, Jesse and Khademi, April and Mahbod, Amirreza and Wang, Chunliang and McKinley, Richard and Wagner, Franca and Muschelli, John and Sweeney, Elizabeth and Roura, Eloy and Lladó, Xavier and Santos, Michel M. and Santos, Wellington P. and Silva-Filho, Abel G. and Tomas-Fernandez, Xavier and Urien, Hélène and Bloch, Isabelle and Valverde, Sergi and Cabezas, Mariano and Vera-Olmos, Francisco Javier and Malpica, Norberto and Guttmann, Charles and Vukusic, Sandra and Edan, Gilles and Dojat, Michel and Styner, Martin and Warfield, Simon K. and Cotton, François and Barillot, Christian},
  date = {2018-09-12},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume = {8},
  number = {1},
  pages = {13650},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/s41598-018-31911-7},
  url = {https://www.nature.com/articles/s41598-018-31911-7},
  urldate = {2022-02-04},
  abstract = {We present a study of multiple sclerosis segmentation algorithms conducted at the international MICCAI 2016 challenge. This challenge was operated using a new open-science computing infrastructure. This allowed for the automatic and independent evaluation of a large range of algorithms in a fair and completely automatic manner. This computing infrastructure was used to evaluate thirteen methods of MS lesions segmentation, exploring a broad range of state-of-theart algorithms, against a high-quality database of 53 MS cases coming from four centers following a common definition of the acquisition protocol. Each case was annotated manually by an unprecedented number of seven different experts. Results of the challenge highlighted that automatic algorithms, including the recent machine learning methods (random forests, deep learning, …), are still trailing human expertise on both detection and delineation criteria. In addition, we demonstrate that computing a statistically robust consensus of the algorithms performs closer to human expertise on one score (segmentation) although still trailing on detection scores.},
  issue = {1},
  langid = {english},
  keywords = {Biomedical engineering,Computational science,Multiple sclerosis,Software},
  file = {/Users/xujinliu/Zotero/storage/Q3WFW3C4/Commowick et al. - 2018 - Objective Evaluation of Multiple Sclerosis Lesion .pdf;/Users/xujinliu/Zotero/storage/QE98ZGGG/s41598-018-31911-7.html}
}

@online{devlinBERTPretrainingDeep2019,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  date = {2019-05-24},
  eprint = {1810.04805},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1810.04805},
  urldate = {2022-02-20},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/xujinliu/Zotero/storage/46TPT39Z/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf;/Users/xujinliu/Zotero/storage/SFCLMWMA/1810.html}
}

@inproceedings{dorentLearningJointLesion2018,
  title = {Learning Joint Lesion and Tissue Segmentation from Task-Specific Hetero-Modal Data Sets},
  author = {Dorent, Reuben and Li, Wenqi and Ekanayake, Jinendra and Ourselin, Sebastien and Vercauteren, Tom},
  date = {2018-12-13},
  url = {https://openreview.net/forum?id=HJeZW_QxxN},
  urldate = {2022-02-04},
  abstract = {Brain tissue segmentation from multimodal MRI is a key building block of many neuroscience analysis pipelines. It could also play an important role in many clinical imaging scenarios. Established...},
  eventtitle = {International {{Conference}} on {{Medical Imaging}} with {{Deep Learning}} -- {{Full Paper Track}}},
  langid = {english},
  file = {/Users/xujinliu/Zotero/storage/3XBGMRXA/Dorent et al. - 2018 - Learning joint lesion and tissue segmentation from.pdf}
}

@online{junMedicalTransformerUniversal2021,
  title = {Medical {{Transformer}}: {{Universal Brain Encoder}} for {{3D MRI Analysis}}},
  shorttitle = {Medical {{Transformer}}},
  author = {Jun, Eunji and Jeong, Seungwoo and Heo, Da-Woon and Suk, Heung-Il},
  date = {2021-04-28},
  eprint = {2104.13633},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2104.13633},
  urldate = {2022-02-07},
  abstract = {Transfer learning has gained attention in medical image analysis due to limited annotated 3D medical datasets for training data-driven deep learning models in the real world. Existing 3D-based methods have transferred the pre-trained models to downstream tasks, which achieved promising results with only a small number of training samples. However, they demand a massive amount of parameters to train the model for 3D medical imaging. In this work, we propose a novel transfer learning framework, called Medical Transformer, that effectively models 3D volumetric images in the form of a sequence of 2D image slices. To make a high-level representation in 3D-form empowering spatial relations better, we take a multi-view approach that leverages plenty of information from the three planes of 3D volume, while providing parameter-efficient training. For building a source model generally applicable to various tasks, we pre-train the model in a self-supervised learning manner for masked encoding vector prediction as a proxy task, using a large-scale normal, healthy brain magnetic resonance imaging (MRI) dataset. Our pre-trained model is evaluated on three downstream tasks: (i) brain disease diagnosis, (ii) brain age prediction, and (iii) brain tumor segmentation, which are actively studied in brain MRI research. The experimental results show that our Medical Transformer outperforms the state-of-the-art transfer learning methods, efficiently reducing the number of parameters up to about 92\% for classification and},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/xujinliu/Zotero/storage/JDW4LVBW/Jun et al. - 2021 - Medical Transformer Universal Brain Encoder for 3.pdf;/Users/xujinliu/Zotero/storage/IDGYTYCE/2104.html}
}

@online{vaswaniAttentionAllYou2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  date = {2017-12-05},
  eprint = {1706.03762},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1706.03762},
  urldate = {2022-02-20},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/xujinliu/Zotero/storage/98FQZGRB/Vaswani et al. - 2017 - Attention Is All You Need.pdf;/Users/xujinliu/Zotero/storage/J8KIB2KN/1706.html}
}

@online{zhuangSelfsupervisedFeatureLearning2019,
  title = {Self-Supervised {{Feature Learning}} for {{3D Medical Images}} by {{Playing}} a {{Rubik}}'s {{Cube}}},
  author = {Zhuang, Xinrui and Li, Yuexiang and Hu, Yifan and Ma, Kai and Yang, Yujiu and Zheng, Yefeng},
  date = {2019-10-05},
  eprint = {1910.02241},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  url = {http://arxiv.org/abs/1910.02241},
  urldate = {2022-02-20},
  abstract = {Witnessed the development of deep learning, increasing number of studies try to build computer aided diagnosis systems for 3D volumetric medical data. However, as the annotations of 3D medical data are difficult to acquire, the number of annotated 3D medical images is often not enough to well train the deep learning networks. The self-supervised learning deeply exploiting the information of raw data is one of the potential solutions to loose the requirement of training data. In this paper, we propose a self-supervised learning framework for the volumetric medical images. A novel proxy task, i.e., Rubik's cube recovery, is formulated to pre-train 3D neural networks. The proxy task involves two operations, i.e., cube rearrangement and cube rotation, which enforce networks to learn translational and rotational invariant features from raw 3D data. Compared to the train-from-scratch strategy, fine-tuning from the pre-trained network leads to a better accuracy on various tasks, e.g., brain hemorrhage classification and brain tumor segmentation. We show that our self-supervised learning approach can substantially boost the accuracies of 3D deep learning networks on the volumetric medical datasets without using extra data. To our best knowledge, this is the first work focusing on the self-supervised learning of 3D neural networks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/Users/xujinliu/Zotero/storage/SPQDBWML/Zhuang et al. - 2019 - Self-supervised Feature Learning for 3D Medical Im.pdf;/Users/xujinliu/Zotero/storage/NZMA56MY/1910.html}
}


