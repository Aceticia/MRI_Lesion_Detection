{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "995bec26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import nibabel as nib\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2b159dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torchvision import transforms\n",
    "\n",
    "from torch.utils.data import Dataset, TensorDataset, random_split, SubsetRandomSampler, ConcatDataset\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error as mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9ac1a1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the GPU if you have one\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2a7ef7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#single scale\n",
    "class fc_layer(nn.Module):\n",
    "    def __init__(self, in_layer, out_layer):\n",
    "        super(fc_layer, self).__init__()\n",
    "        self.fc = nn.Linear(in_num, out_layer)\n",
    "        #if in_features=5 and out_features=10 and the input tensor x \n",
    "        #has dimensions 2-3-5, then the output tensor will have dimensions 2-3-10???\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "class avg_pool(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(avg_pool, self).__init__()\n",
    "        self.avgp = nn.AvgPool3d(2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.avgp(x)\n",
    "\n",
    "class relu_act(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(relu_act, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.relu(x)\n",
    "\n",
    "class softmax_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(softmax_layer, self).__init__()\n",
    "        self.softmax = nn.Softmax()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20e1f75",
   "metadata": {},
   "source": [
    "# Age prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "263fe02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class age_pred(nn.Module):\n",
    "    def __init__(self, in_layer, out_layer):\n",
    "        super(age_pred, self).__init__()\n",
    "        self.in_layer = in_layer\n",
    "        self.out_layer = out_layer\n",
    "        self.layer1 = fc_layer(in_layer, out_layer)\n",
    "        self.avg = avg_pool()\n",
    "        self.relu = relu_act()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x0 = layer1(x)\n",
    "        x1 = avg(x0)\n",
    "        x2 = relu_act(x1)\n",
    "        return x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8f6a8f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss for regression\n",
    "def get_mae(model, pred, y_test):\n",
    "    mae_out = mae(y_pred=pred, y_true=y_test)\n",
    "    return mae_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "da78d89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ADNI_Dataset_regression(Dataset):\n",
    "    def __init__(self, root_dir, data_file):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory of all the images.\n",
    "            data_file (string): File name of the train/test split file.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.data_file = data_file\n",
    "    \n",
    "    def __len__(self):\n",
    "        return sum(1 for line in open(self.data_file))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        df = open(self.data_file)\n",
    "        lines = df.readlines()\n",
    "        lst = lines[idx].split()\n",
    "        img_name = lst[0]\n",
    "        img_label = lst[4]  #age\n",
    "        print(img_label)\n",
    "        image_path = os.path.join(self.root_dir, img_name)\n",
    "        image = nib.load(image_path)\n",
    "        a = np.array(image.dataobj) #convert to np array\n",
    "        \n",
    "        sample = {'image': a, 'label': img_label}\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "23998249",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCH = 100\n",
    "BATCH_SIZE = 10\n",
    "LR = 0.0001\n",
    "SAVE_PATH_AP = r'C:\\Users\\pbhav\\Desktop\\NYU\\ivp\\project\\model\\AP' #path to save model\n",
    "SAVE_PATH_BC = r'C:\\Users\\pbhav\\Desktop\\NYU\\ivp\\project\\model\\BC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "83f7e846",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(net, data_loader, optimizer, criterion, epoch):\n",
    "    net.train()\n",
    "    loss_stat = []\n",
    "    for i, img_label in enumerate(data_loader):\n",
    "        img, label = img_label\n",
    "        img = img.to(device=device, dtype=torch.float32)\n",
    "        \n",
    "        pred = net(img)\n",
    "        loss = criterion(label, pred)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_stat += [loss.item()]/len(data_loader) * 100\n",
    "    \n",
    "    print (\"Epoch {}: [{}/{}] Loss: {:.3f}\".format(epoch, len(data_loader), len(data_loader),np.mean(loss_stat))) \n",
    "    \n",
    "    return np.mean(loss_stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f24dec23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_epoch(net, data_loader, criterion, epoch):\n",
    "    net.eval()\n",
    "\n",
    "    val_loss_stat = []\n",
    "    for i, img_label in enumerate(data_loader):\n",
    "        img, label = img_label\n",
    "        img = img.to(device=device, dtype=torch.float32)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pred = net(img)\n",
    "            val_loss = criterion(label, pred)\n",
    "      \n",
    "        val_loss_stat += [val_loss.item()]/len(data_loader) * 100\n",
    "        \n",
    "    print (\"Val Loss: {:.3f} \".format(np.mean(val_loss_stat)))\n",
    "    \n",
    "    return np.mean(val_loss_stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "97743bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_data = ADNI_Dataset_regression(r\"C:/Users/pbhav/Desktop/NYU/ivp/project/ADNI/\", \"C:/Users/pbhav/Downloads/ADNI1_Annual_2_Yr_3T_4_23_2022.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9f4412",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_size = ... #size after the transformer\n",
    "net = age_pred(in_size, 1)\n",
    "net.to(device)  # run net.to(device) if using GPU\n",
    "print(net)`\n",
    "\n",
    "n_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print('Number of parameters in network: ', n_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4b9414",
   "metadata": {},
   "outputs": [],
   "source": [
    "#kfold validation for k = 5\n",
    "k=5\n",
    "splits=KFold(n_splits=k,shuffle=True,random_state=42)\n",
    "\n",
    "# criterion = get_mae()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3467477",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold(model, dataset, NUM_EPOCH, LR, BATCH_SIZE, SAVE_PATH, criterion):\n",
    "    foldperf={}\n",
    "    for fold, (train_idx, val_idx) in enumerate(splits.split(np.arange(len(dataset)))):\n",
    "\n",
    "        print('Fold {}'.format(fold + 1))\n",
    "\n",
    "        train_sampler = SubsetRandomSampler(train_idx)\n",
    "        test_sampler = SubsetRandomSampler(val_idx)\n",
    "        train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, sampler=train_sampler)\n",
    "        test_loader = DataLoader(dataset, batch_size=BATCH_SIZE, sampler=test_sampler)\n",
    "\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        model = model()\n",
    "        model.to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "        history = {'train_loss': [], 'test_loss': [],'train_acc':[],'test_acc':[]}\n",
    "\n",
    "        for epoch in range(NUM_EPOCH):\n",
    "            train_loss, train_correct=train_epoch(model, train_loader, optimizer, criterion, epoch)\n",
    "            test_loss, test_correct=valid_epoch(model, test_loader, criterion, epoch)\n",
    "\n",
    "            train_loss = train_loss / len(train_loader.sampler)\n",
    "            train_acc = train_correct / len(train_loader.sampler) * 100\n",
    "            test_loss = test_loss / len(test_loader.sampler)\n",
    "            test_acc = test_correct / len(test_loader.sampler) * 100\n",
    "\n",
    "            print(\"Epoch:{}/{} AVG Training Loss:{:.3f} AVG Test Loss:{:.3f} AVG Training Acc {:.2f} % AVG Test Acc {:.2f} %\".format(epoch + 1,\n",
    "                                                                                                                 num_epochs,\n",
    "                                                                                                                 train_loss,\n",
    "                                                                                                                 test_loss,\n",
    "                                                                                                                 train_acc,\n",
    "                                                                                                                 test_acc))\n",
    "            history['train_loss'].append(train_loss)\n",
    "            history['test_loss'].append(test_loss)\n",
    "            history['train_acc'].append(train_acc)\n",
    "            history['test_acc'].append(test_acc)\n",
    "\n",
    "    #         Save the model after each epoch\n",
    "            if os.path.isdir(SAVE_PATH):\n",
    "                torch.save(net.state_dict(),SAVE_PATH + 'agepredepoch{}.pth'.format(epoch + 1))\n",
    "            else:\n",
    "                os.makedirs(model_save_path, exist_ok=True)\n",
    "                torch.save(net.state_dict(),SAVE_PATH + 'agepredepoch{}.pth'.format(epoch + 1))\n",
    "            print('Checkpoint {} saved to {}'.format(epoch + 1, SAVE_PATH + 'agepredepoch{}.pth'.format(epoch + 1)))   \n",
    "        foldperf['fold{}'.format(fold+1)] = history  \n",
    "\n",
    "    torch.save(model,'k_cross.pt')\n",
    "    return foldperf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44b139c",
   "metadata": {},
   "outputs": [],
   "source": [
    "foldperf1 = k_fold(age_pred, regression_data, NUM_EPOCH, LR, BATCH_SIZE, SAVE_PATH_AP, criterion=get_mae())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a324f966",
   "metadata": {},
   "outputs": [],
   "source": [
    "testl_f, tl_f, testa_f, ta_f = [], [], [], []\n",
    "\n",
    "for f in range(1,k+1):\n",
    "    tl_f.append(np.mean(foldperf1['fold{}'.format(f)]['train_loss']))\n",
    "    testl_f.append(np.mean(foldperf1['fold{}'.format(f)]['test_loss']))\n",
    "    ta_f.append(np.mean(foldperf1['fold{}'.format(f)]['train_acc']))\n",
    "    testa_f.append(np.mean(foldperf1['fold{}'.format(f)]['test_acc']))\n",
    "\n",
    "print('Performance of {} fold cross validation'.format(k))\n",
    "print(\"Average Training Loss: {:.3f} \\t Average Test Loss: {:.3f} \\t Average Training Acc: {:.2f} \\t Average Test Acc: {:.2f}\".format(np.mean(tl_f), np.mean(testl_f), np.mean(ta_f), np.mean(testa_f)))     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5679d12b",
   "metadata": {},
   "source": [
    "# Brain disease classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7dfb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data prep\n",
    "class ADNI_Dataset_classification(Dataset):\n",
    "    def __init__(self, root_dir, data_file):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory of all the images.\n",
    "            data_file (string): File name of the train/test split file.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.data_file = data_file\n",
    "    \n",
    "    def __len__(self):\n",
    "        return sum(1 for line in open(self.data_file))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        df = open(self.data_file)\n",
    "        lines = df.readlines()\n",
    "        lst = lines[idx].split()\n",
    "        img_name = lst[1]\n",
    "        img_label = lst[2]\n",
    "        image_path = os.path.join(self.root_dir, img_name)\n",
    "        image = nib.load(image_path)\n",
    "        a = np.array(image.dataobj) #convert to np array\n",
    "        \n",
    "        if img_label == 'CN': #Cognitive Normal\n",
    "            label = 0\n",
    "        elif img_label == 'AD': #Alzheimer's \n",
    "            label = 1\n",
    "        elif img_label == 'MCI': #Mild Cognitive Impairement\n",
    "            label = 2\n",
    "\n",
    "        sample = {'image': a, 'label': label}\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b527f77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss function for classification\n",
    "def a_value(probabilities, zero_label=0, one_label=1):\n",
    "    \"\"\"\n",
    "    Approximates the AUC by the method described in Hand and Till 2001,\n",
    "    equation 3.\n",
    "    NB: The class labels should be in the set [0,n-1] where n = # of classes.\n",
    "    The class probability should be at the index of its label in the\n",
    "    probability list.\n",
    "    I.e. With 3 classes the labels should be 0, 1, 2. The class probability\n",
    "    for class '1' will be found in index 1 in the class probability list\n",
    "    wrapped inside the zipped list with the labels.\n",
    "    Args:\n",
    "        probabilities (list): A zipped list of the labels and the\n",
    "            class probabilities in the form (m = # data instances):\n",
    "             [(label1, [p(x1c1), p(x1c2), ... p(x1cn)]),\n",
    "              (label2, [p(x2c1), p(x2c2), ... p(x2cn)])\n",
    "                             ...\n",
    "              (labelm, [p(xmc1), p(xmc2), ... (pxmcn)])\n",
    "             ]\n",
    "        zero_label (optional, int): The label to use as the class '0'.\n",
    "            Must be an integer, see above for details.\n",
    "        one_label (optional, int): The label to use as the class '1'.\n",
    "            Must be an integer, see above for details.\n",
    "    Returns:\n",
    "        The A-value as a floating point.\n",
    "    \"\"\"\n",
    "    # Obtain a list of the probabilities for the specified zero label class\n",
    "    expanded_points = []\n",
    "    for instance in probabilities:\n",
    "        if instance[0] == zero_label or instance[0] == one_label:\n",
    "            expanded_points.append((instance[0], instance[1][zero_label]))\n",
    "    sorted_ranks = sorted(expanded_points, key=lambda x: x[1])\n",
    "\n",
    "    n0, n1, sum_ranks = 0, 0, 0\n",
    "    # Iterate through ranks and increment counters for overall count and ranks of class 0\n",
    "    for index, point in enumerate(sorted_ranks):\n",
    "        if point[0] == zero_label:\n",
    "            n0 += 1\n",
    "            sum_ranks += index + 1  # Add 1 as ranks are one-based\n",
    "        elif point[0] == one_label:\n",
    "            n1 += 1\n",
    "        else:\n",
    "            pass  # Not interested in this class\n",
    "\n",
    "    return (sum_ranks - (n0*(n0+1)/2.0)) / float(n0 * n1)  # Eqn 3\n",
    "\n",
    "def get_mAUC(data, num_classes):\n",
    "    \"\"\"\n",
    "    Calculates the MAUC over a set of multi-class probabilities and\n",
    "    their labels. This is equation 7 in Hand and Till's 2001 paper.\n",
    "    NB: The class labels should be in the set [0,n-1] where n = # of classes.\n",
    "    The class probability should be at the index of its label in the\n",
    "    probability list.\n",
    "    I.e. With 3 classes the labels should be 0, 1, 2. The class probability\n",
    "    for class '1' will be found in index 1 in the class probability list\n",
    "    wrapped inside the zipped list with the labels.\n",
    "    Args:\n",
    "        data (list): A zipped list (NOT A GENERATOR) of the labels and the\n",
    "            class probabilities in the form (m = # data instances):\n",
    "             [(label1, [p(x1c1), p(x1c2), ... p(x1cn)]),\n",
    "              (label2, [p(x2c1), p(x2c2), ... p(x2cn)])\n",
    "                             ...\n",
    "              (labelm, [p(xmc1), p(xmc2), ... (pxmcn)])\n",
    "             ]\n",
    "        num_classes (int): The number of classes in the dataset.\n",
    "    Returns:\n",
    "        The MAUC as a floating point value.\n",
    "    \"\"\"\n",
    "    # Find all pairwise comparisons of labels\n",
    "    class_pairs = [x for x in itertools.combinations(xrange(num_classes), 2)]\n",
    "\n",
    "    # Have to take average of A value with both classes acting as label 0 as this\n",
    "    # gives different outputs for more than 2 classes\n",
    "    sum_avals = 0\n",
    "    for pairing in class_pairs:\n",
    "        sum_avals += (a_value(data, zero_label=pairing[0], one_label=pairing[1]) +\n",
    "                      a_value(data, zero_label=pairing[1], one_label=pairing[0])) / 2.0\n",
    "\n",
    "    return sum_avals * (2 / float(num_classes * (num_classes-1)))  # Eqn 7\n",
    "\n",
    "#CITE THE PAPER AND GITHUB https://gist.github.com/stulacy/672114792371dc13b247"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9a37c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BD_classify(nn.Module):\n",
    "    def __init__(self, in_layer, out_layer):\n",
    "        super(BD_classify, self).__init__()\n",
    "        self.in_layer = in_layer\n",
    "        self.out_layer = out_layer\n",
    "        self.layer1 = fc_layer(in_layer, out_layer)\n",
    "        self.avg = avg_pool()\n",
    "        self.softmax = softmax_layer()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x0 = layer1(x)\n",
    "        x1 = avg(x0)\n",
    "        x2 = softmax_layer(x1)\n",
    "        return x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b24344",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_size = ...#size after the transformer\n",
    "net1 = BD_classify(in_size, 3)\n",
    "net1.to(device)  # run net.to(device) if using GPU\n",
    "print(net1)\n",
    "\n",
    "n_params1 = sum(p.numel() for p in net1.parameters() if p.requires_grad)\n",
    "print('Number of parameters in network: ', n_params1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0a16fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "foldperf2 = k_fold(BD_classify, ADNI_Dataset_classification, NUM_EPOCH, LR, BATCH_SIZE, SAVE_PATH_BC, criterion=get_mAUC())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438d71ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "testl_f1, tl_f1, testa_f1, ta_f1 = [], [], [], []\n",
    "\n",
    "for f in range(1,k+1):\n",
    "    tl_f1.append(np.mean(foldperf2['fold{}'.format(f)]['train_loss']))\n",
    "    testl_f1.append(np.mean(foldperf2['fold{}'.format(f)]['test_loss']))\n",
    "    ta_f1.append(np.mean(foldperf2['fold{}'.format(f)]['train_acc']))\n",
    "    testa_f1.append(np.mean(foldperf2['fold{}'.format(f)]['test_acc']))\n",
    "\n",
    "print('Performance of {} fold cross validation'.format(k))\n",
    "print(\"Average Training Loss: {:.3f} \\t Average Test Loss: {:.3f} \\t Average Training Acc: {:.2f} \\t Average Test Acc: {:.2f}\".format(np.mean(tl_f1), np.mean(testl_f1), np.mean(ta_f1), np.mean(testa_f1)))     \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
